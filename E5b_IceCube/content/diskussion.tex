\section{Diskussion}
\label{sec:Diskussion}

Es wurden drei verschiedene binäre Klassifikatoren untersucht: der Naive-Bayes-, der k-Nearest-Neighbor- und der Random-Forest-Klassifikator. Als Trainingsdaten wurden jeweils 30000 Events und zum Testen 10000 Events verwendet (Signal und Background jeweils im Verhältnis 1:1), die durch eine Monte-Carlo-Simulation generiert wurden. Dabei schloss der Random-Forest mit Abstand am besten ab: Mit ihm lassen sich bei einer der Anwendung angemessenen Wahl des Scorecuts praktisch beliebig hohe Präzisionen erreichen. Die Kosten dafür sind eine entsprechend niedrigere Effizienz, die aber dennoch durch die beiden anderen untersuchten Lerner bei vergleichbar hohen Präzisionen geschlagen werden kann. Damit einher geht auch eine entsprechend hoher Flächeninhalt im ROC-Diagramm von 0.98. Hinzu kommt noch, dass die Laufzeit bei diesem Lerner am kleinsten verglichen mit den anderen Klassifikatoren war. Innerhalb von unter $14 \, \text{s}$ konnte der komplette Datensatz in Trainings- und  Testevents unterteilt werden, zum Training verwendet werden und es konnten für die 10000 Events jeweils eine Wahrscheinlichkeit angegeben werden, mit der das jeweilige Event den Signalen oder dem Background zugeordnet werden kann. Die Schlussfolgerung lautet also, dass der Random-Forest-Klassifikator für diese Anwendung eindeutig einem Algorithmus auf der Grundlage von Naive-Bayes oder k-Nearest-Neighbor vorzuziehen ist.

Ein weiterer noch nicht genannter Vorteil ergibt sich aus der Überprüfbarkeit der Relevanz der ausgewählten physikalischen Attribute für diese Anwendung. Dies war nur mit dem Random-Forest-Klassifikator möglich. Es stellte sich heraus, dass gerade die Features wichtig sind, in denen der Zenitwinkel oder Geschwindigkeiten und Zeiten für Bewegungen in die $z$-Richtung angegeben sind. Dies lässt sich intuitiv nachvollziehen: Signal- und Backgroundevents unterscheiden sich gerade durch die Herkunft der beobachteten Sekundärteilchen.

Verbesserungsmöglichkeiten der Datenanalyse bestehen zum einen in einer besseren Auswertung der vorhandenen Daten als auch in einer schlichten Erhöhung der Trainings- und Testdatenmenge. Bezüglich einer qualitativ besseren Datenanalyse könnten noch andere Klassifikatoren für das supervised learning untersucht werden oder die bereits verwendeten Lerner besser an das vorliegende Problem angepasst werden. So hätte es beispielsweise mit Sicherheit sinnvollere Methoden zur Feature-Selection anstelle von \texttt{SelectKBest} gegeben: Redundanzen in den ausgewählten Features konnten so beispielsweise nicht eliminiert werden. Das wäre z.B. durch eine mRMR-Feature-Selection möglich gewesen. Anschließend wäre es auch interessant gewesen, die Korrelation von der Anzahl der gewählten Attribute in der Feature-Selection (hier: 26) mit der Anzahl der Bäume über die im Random-Forest gemittelt wird mit den Bewertungskriterien für Lerner herauszuarbeiten. Ein weiterer interessanter Vergleich wäre durch den Einsatz eines neuronalen Netzes möglich gewesen, welche im Rahmen dieses Versuchs nicht betrachtet wurden.